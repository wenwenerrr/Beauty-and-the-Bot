{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip -q install --upgrade openai pandas tqdm tiktoken"
      ],
      "metadata": {
        "id": "IJpDOSP7Pfog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "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\n",
        "'''"
      ],
      "metadata": {
        "id": "x8-S679O3OZ6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "12282a27-1457-41fe-c2c4-0ca9068a65da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n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\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# - Reads /content/try_example.json (\n",
        "# - For each product, sends metadata + MAIN image URL to gpt-4o-mini\n",
        "# - Writes one JSON line per product to /content/synthetic_intents.jsonl\n",
        "# - Keeps outputs short, diverse, and *not* copied verbatim from titles\n",
        "# ---------------------------------------------------------------------\n",
        "# Usage:\n",
        "#   1) Upload try_example.json to Colab (left sidebar → Files → Upload)\n",
        "#   2) Run this cell; when prompted, paste OpenAI key (we do NOT print it)\n",
        "#   3) Inspect /content/synthetic_intents.jsonl\n",
        "\n",
        "!pip -q install --upgrade openai pandas tqdm\n",
        "\n",
        "import os, json, time, re\n",
        "import random # Import random for sampling\n",
        "from typing import Optional, Dict, Any, List\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from openai import OpenAI\n",
        "\n",
        "# ------- Config -------\n",
        "JSONL_PATH = \"/content/summarized_for_review_merged_final.jsonl\"\n",
        "OUT_PATH   = \"/content/synthetic_intents.jsonl\"\n",
        "MODEL      = \"gpt-4o\"                     # vision-capable; correct id (letter 'o')\n",
        "MAX_ITEMS = 3000                             # limit for quick tests; set to None for all\n",
        "INTENTS_PER_ITEM = 5                           # 4–6 is a good range\n",
        "CONSTRAINT_SETS   = 1\n",
        "RANDOM_SEED = 42 # Define a random seed for reproducibility\n",
        "\n",
        "# Paste key when prompted (safer than hard-coding)\n",
        "if \"OPENAI_API_KEY\" not in os.environ or not os.environ[\"OPENAI_API_KEY\"]:\n",
        "    import getpass\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Paste your OpenAI API key (hidden): \")\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "def read_jsonl(path: str):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                yield json.loads(line)\n",
        "\n",
        "def select_main_image(images_field) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Accepts list[dict] with {hi_res|large|thumb, variant}, list[str], str, or None.\n",
        "    Prefer variant=='MAIN', then first hi_res/large/thumb URL.\n",
        "    \"\"\"\n",
        "    if not images_field:\n",
        "        return None\n",
        "    items = []\n",
        "    if isinstance(images_field, str):\n",
        "        items = [{\"variant\":\"MAIN\", \"hi_res\":images_field}]\n",
        "    elif isinstance(images_field, list):\n",
        "        for it in images_field:\n",
        "            if isinstance(it, str):\n",
        "                items.append({\"variant\":\"\", \"hi_res\":it})\n",
        "            elif isinstance(it, dict):\n",
        "                items.append(it)\n",
        "    # prefer MAIN\n",
        "    def first_url(d: Dict[str,Any]):\n",
        "        return d.get(\"hi_res\") or d.get(\"large\") or d.get(\"thumb\") or d.get(\"url\")\n",
        "    mains = [d for d in items if str(d.get(\"variant\",\"\")).upper()==\"MAIN\"]\n",
        "    for d in mains + items:\n",
        "        url = first_url(d)\n",
        "        if isinstance(url, str) and url.startswith(\"http\"):\n",
        "            return url\n",
        "    return None\n",
        "\n",
        "def compact_details(details: Any) -> str:\n",
        "    \"\"\"Turn details dict into a compact 'Key: Value' string; keep a few useful keys.\"\"\"\n",
        "    if not isinstance(details, dict):\n",
        "        return \"\"\n",
        "    keep = [\"Brand\",\"Style\",\"Color\",\"Material\",\"Size\",\"Item Form\",\"Hair Type\", \"Age Range\", \"Material Feature\", \"Skin Type\"]\n",
        "    parts = []\n",
        "    for k in keep:\n",
        "        v = details.get(k)\n",
        "        if v:\n",
        "            parts.append(f\"{k}: {v}\")\n",
        "    return \" | \".join(parts)\n",
        "\n",
        "SYSTEM_MSG = (\n",
        "    \"You create short, diverse, natural shopping intents for a beauty product dataset. \"\n",
        "    \"DO NOT copy the title verbatim. Avoid brand/product codes unless asked. \"\n",
        "    \"Write intents a real shopper could type (6–14 words).\"\n",
        ")\n",
        "\n",
        "def build_user_parts(product: Dict[str,Any], main_image_url: Optional[str]) -> List[Dict[str,Any]]:\n",
        "    \"\"\"Compose a multimodal message with text + optional image_url.\"\"\"\n",
        "    title   = product.get(\"title\",\"\")\n",
        "    summary = product.get(\"summary\",\"\")\n",
        "    features = product.get(\"features\",[]) or []\n",
        "    description = product.get(\"description\",[]) or []\n",
        "    details = product.get(\"details\",{}) or {}\n",
        "    price   = product.get(\"price\", None)\n",
        "    avg_rating = product.get(\"avg_rating\", product.get(\"average_rating\", None))\n",
        "    rating_number = product.get(\"rating_number\", product.get(\"total_reviews\", None))\n",
        "    main_category = product.get(\"main_category\", \"\")\n",
        "    brand = details.get(\"Brand\") or product.get(\"store\") or \"\"\n",
        "\n",
        "    # keep it concise\n",
        "    feat_txt = \" • \".join([str(x) for x in features][:5])\n",
        "    desc_txt = \" \".join([str(x) for x in description][:2])[:500]\n",
        "    det_txt  = compact_details(details)\n",
        "\n",
        "    meta = {\n",
        "        \"parent_asin\": product.get(\"parent_asin\", \"\"),\n",
        "        \"title\": title,\n",
        "        \"summary\": summary,\n",
        "        \"main_category\": main_category,\n",
        "        \"brand\": brand,\n",
        "        \"price\": price,\n",
        "        \"avg_rating\": avg_rating,\n",
        "        \"rating_number\": rating_number,\n",
        "        \"features\": feat_txt,\n",
        "        \"description\": desc_txt,\n",
        "        \"details_compact\": det_txt\n",
        "    }\n",
        "\n",
        "    text_block = (\n",
        "        \"You will produce synthetic *user shopping intents* and *structured constraint sets* \"\n",
        "    \"for the following product. Intents should be compatible with the product and avoid copying exact phrasing.\\n\\n\"\n",
        "    f\"PRODUCT METADATA (JSON):\\n{json.dumps(meta, ensure_ascii=False)}\\n\\n\"\n",
        "    \"Output JSON schema:\\n\"\n",
        "    \"{\\n\"\n",
        "    '  \"intents\": [short natural language queries, 5 items],\\n'\n",
        "    '  \"constraints\": [  // 1 object, optional fields if unknown\\n'\n",
        "    '     {\"brand\": string, \"price_min\": number, \"price_max\": number, \"rating_min\": number}\\n'\n",
        "    \"  ]\\n\"\n",
        "    \"}\\n\"\n",
        "    \"Rules:\\n\"\n",
        "    \"- Rephrase with synonyms; avoid any exact phrase of length ≥ 2 words from title/features/description.\\n\"\n",
        "    \"- Intents: diverse tone & vocabulary; 6–14 tokens; no brand unless natural.\\n\"\n",
        "    \"- If price exists, set a plausible [price_min, price_max] around it (e.g., ±20%), make sure the actual price is within the range.\\n\"\n",
        "    \"- If average_rating exists, set rating_min to an integer value equal to or below it (e.g., 4.0).\\n\"\n",
        "    \"- If brand exists, set brand to the corresponding brand.\"\n",
        "    \"- If price/brand/rating is unknown or missing (e.g., null/None/NA/N/A/\\\"nah\\\"/empty), DO NOT include that field in the constraints object.\\n\"\n",
        "    \"- Keep outputs concise. Return ONLY valid JSON.\"\n",
        "\n",
        "    )\n",
        "\n",
        "    parts = [{\"type\":\"text\",\"text\":text_block}]\n",
        "    if main_image_url:\n",
        "        parts.append({\"type\":\"image_url\",\"image_url\":{\"url\": main_image_url}})\n",
        "    return parts\n",
        "\n",
        "def call_llm(product: Dict[str,Any], main_image_url: Optional[str]) -> Dict[str,Any]:\n",
        "    \"\"\"Calls gpt-4o-mini with text+image to generate intents + constraints. Returns parsed JSON dict.\"\"\"\n",
        "    messages = [\n",
        "        {\"role\":\"system\",\"content\":[{\"type\":\"text\",\"text\": SYSTEM_MSG}]},\n",
        "        {\"role\":\"user\",\"content\": build_user_parts(product, main_image_url)}\n",
        "    ]\n",
        "    resp = client.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        messages=messages,\n",
        "        temperature=0.7,\n",
        "        max_tokens=600,\n",
        "\n",
        "    )\n",
        "    text = resp.choices[0].message.content\n",
        "\n",
        "    # robust JSON extraction\n",
        "    def extract_json(s: str):\n",
        "        start = s.find(\"{\")\n",
        "        end   = s.rfind(\"}\")\n",
        "        if start == -1 or end == -1 or end <= start:\n",
        "            raise ValueError(\"No JSON object found in response.\")\n",
        "        return json.loads(s[start:end+1])\n",
        "\n",
        "    data = extract_json(text)\n",
        "    # light normalization\n",
        "    intents = [i.strip() for i in data.get(\"intents\", []) if isinstance(i, str) and i.strip()]\n",
        "    intents = intents[:INTENTS_PER_ITEM] or intents\n",
        "    cons    = data.get(\"constraints\", [])\n",
        "    if not isinstance(cons, list): cons = []\n",
        "    cons = cons[:CONSTRAINT_SETS]\n",
        "    return {\"intents\": intents, \"constraints\": cons, \"raw_text\": text}\n",
        "\n",
        "# --------- Run ----------\n",
        "rows = list(read_jsonl(JSONL_PATH))\n",
        "if MAX_ITEMS and MAX_ITEMS < len(rows):\n",
        "    random.seed(RANDOM_SEED) # Set the random seed\n",
        "    rows = random.sample(rows, MAX_ITEMS) # Randomly sample MAX_ITEMS\n",
        "\n",
        "out_f = open(OUT_PATH, \"w\", encoding=\"utf-8\")\n",
        "empty_constraints_count = 0  # Initialize the counter\n",
        "processed_count = 0 # Initialize a counter for processed items\n",
        "try:\n",
        "    for rec in tqdm(rows, desc=\"Generating synthetic intents\"):\n",
        "        main_img = select_main_image(rec.get(\"images\"))\n",
        "        result = {}\n",
        "        try:\n",
        "            result = call_llm(rec, main_img)\n",
        "        except Exception as e:\n",
        "            # retry once without image if vision fails\n",
        "            try:\n",
        "                result = call_llm(rec, None)\n",
        "            except Exception as e2:\n",
        "                result = {\"intents\": [], \"constraints\": [], \"error\": f\"{type(e2).__name__}: {e2}\"}\n",
        "\n",
        "        # Check if constraints list is empty and increment counter\n",
        "        if 'constraints' in result and isinstance(result['constraints'], list) and len(result['constraints']) == 0:\n",
        "            empty_constraints_count += 1\n",
        "\n",
        "        line = {\n",
        "            \"parent_asin\": rec.get(\"parent_asin\"),\n",
        "            \"title\": rec.get(\"title\"),\n",
        "            \"brand\": (rec.get(\"details\") or {}).get(\"Brand\") or rec.get(\"store\"),\n",
        "            \"main_category\": rec.get(\"main_category\"),\n",
        "            \"main_image\": select_main_image(rec.get(\"images\")),\n",
        "            \"generated\": result\n",
        "        }\n",
        "        out_f.write(json.dumps(line, ensure_ascii=False) + \"\\n\")\n",
        "        out_f.flush()\n",
        "        time.sleep(0.4)  # gentle pacing\n",
        "\n",
        "        processed_count += 1\n",
        "        if processed_count % 100 == 0:  # Report every 100 items\n",
        "            print(f\"Processed {processed_count} items. Empty constraints so far: {empty_constraints_count}\")\n",
        "\n",
        "finally:\n",
        "    out_f.close()\n",
        "\n",
        "print(f\"✔ Wrote {OUT_PATH}\")\n",
        "print(f\"Total number of products with empty generated constraints: {empty_constraints_count}\") # Report the final count\n",
        "# Peek a few lines\n",
        "for i, line in zip(range(3), read_jsonl(OUT_PATH)):\n",
        "    print(json.dumps(line, ensure_ascii=False)[:500] + (\"...\" if len(json.dumps(line, ensure_ascii=False))>500 else \"\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zl07keyHnPY",
        "outputId": "dcfb9d0c-7161-41c4-ebf2-50a6372355fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating synthetic intents:   3%|▎         | 100/3000 [09:55<5:39:45,  7.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 100 items. Empty constraints so far: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating synthetic intents:   7%|▋         | 200/3000 [21:46<4:55:36,  6.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 200 items. Empty constraints so far: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating synthetic intents:  10%|█         | 300/3000 [30:49<5:48:06,  7.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 300 items. Empty constraints so far: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating synthetic intents:  13%|█▎        | 400/3000 [40:41<3:22:57,  4.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 400 items. Empty constraints so far: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating synthetic intents:  17%|█▋        | 500/3000 [50:35<5:58:53,  8.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 500 items. Empty constraints so far: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating synthetic intents:  20%|██        | 600/3000 [1:00:23<4:24:03,  6.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 600 items. Empty constraints so far: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating synthetic intents:  23%|██▎       | 700/3000 [1:08:03<3:49:41,  5.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 700 items. Empty constraints so far: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating synthetic intents:  27%|██▋       | 800/3000 [1:15:57<2:42:44,  4.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 800 items. Empty constraints so far: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating synthetic intents:  30%|███       | 900/3000 [1:24:11<2:44:54,  4.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 900 items. Empty constraints so far: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating synthetic intents:  33%|███▎      | 1000/3000 [1:33:59<2:38:47,  4.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 1000 items. Empty constraints so far: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating synthetic intents:  37%|███▋      | 1100/3000 [1:41:35<2:14:38,  4.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 1100 items. Empty constraints so far: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating synthetic intents:  40%|████      | 1200/3000 [1:49:49<2:58:33,  5.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 1200 items. Empty constraints so far: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating synthetic intents:  43%|████▎     | 1300/3000 [1:58:10<2:51:22,  6.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 1300 items. Empty constraints so far: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating synthetic intents:  47%|████▋     | 1400/3000 [2:07:21<4:14:00,  9.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 1400 items. Empty constraints so far: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating synthetic intents:  50%|█████     | 1500/3000 [2:15:34<1:36:51,  3.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 1500 items. Empty constraints so far: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating synthetic intents:  53%|█████▎    | 1600/3000 [2:23:16<1:18:46,  3.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 1600 items. Empty constraints so far: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating synthetic intents:  57%|█████▋    | 1700/3000 [2:33:38<1:43:49,  4.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 1700 items. Empty constraints so far: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating synthetic intents:  60%|██████    | 1800/3000 [2:42:49<1:22:16,  4.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 1800 items. Empty constraints so far: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating synthetic intents:  63%|██████▎   | 1900/3000 [2:51:41<1:13:52,  4.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 1900 items. Empty constraints so far: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating synthetic intents:  67%|██████▋   | 2000/3000 [3:01:50<1:59:52,  7.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 2000 items. Empty constraints so far: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating synthetic intents:  70%|███████   | 2100/3000 [3:11:53<1:09:09,  4.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 2100 items. Empty constraints so far: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating synthetic intents:  73%|███████▎  | 2200/3000 [3:20:48<1:10:04,  5.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 2200 items. Empty constraints so far: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating synthetic intents:  77%|███████▋  | 2300/3000 [3:29:06<43:37,  3.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 2300 items. Empty constraints so far: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating synthetic intents:  80%|████████  | 2400/3000 [3:38:05<45:16,  4.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 2400 items. Empty constraints so far: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating synthetic intents:  83%|████████▎ | 2500/3000 [3:46:49<33:36,  4.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 2500 items. Empty constraints so far: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating synthetic intents:  87%|████████▋ | 2600/3000 [3:54:45<29:37,  4.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 2600 items. Empty constraints so far: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating synthetic intents:  90%|█████████ | 2700/3000 [4:02:36<21:25,  4.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 2700 items. Empty constraints so far: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating synthetic intents:  93%|█████████▎| 2800/3000 [4:10:47<14:51,  4.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 2800 items. Empty constraints so far: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating synthetic intents:  97%|█████████▋| 2900/3000 [4:20:11<07:36,  4.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 2900 items. Empty constraints so far: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating synthetic intents: 100%|██████████| 3000/3000 [4:30:03<00:00,  5.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 3000 items. Empty constraints so far: 3\n",
            "✔ Wrote /content/synthetic_intents.jsonl\n",
            "Total number of products with empty generated constraints: 3\n",
            "{\"parent_asin\": \"B01FL462KQ\", \"title\": \"Lilah James KP Exfoliating Scrub 8oz - A Natural Mask And Scrub For Keratosis Pilaris\", \"brand\": null, \"main_category\": \"All Beauty\", \"main_image\": \"https://m.media-amazon.com/images/I/41nx+VYWS3L.jpg\", \"generated\": {\"intents\": [\"Best exfoliator for rough skin bumps\", \"Natural scrub for dry skin conditions\", \"Effective keratosis pilaris skin treatment\", \"Gentle exfoliating mask for smooth skin\", \"Looking for a scrub to improve dry skin\"], \"constraints\": [{...\n",
            "{\"parent_asin\": \"B00AQ0RIJK\", \"title\": \"Salon Grf H/S Ex/S Trvl S Size Salon Grafix Extra Super Hold Shaping Hair Spray, 1.5 Oz, Pack of 1\", \"brand\": \"Salon Grafix\", \"main_category\": \"All Beauty\", \"main_image\": \"https://m.media-amazon.com/images/I/61z23vWDUCL._SL1500_.jpg\", \"generated\": {\"intents\": [\"Travel-size strong hold hair spray\", \"Portable hair spray for firm styling\", \"Small super hold hair spray for purse\", \"Extra hold hair spray with pleasant scent\", \"Compact hairspray for touch-ups on...\n",
            "{\"parent_asin\": \"B07ZX4R33J\", \"title\": \"SIQUK 50 Pcs Butterfly Clips 90s Butterfly Hair Clip Alligator Hair Clips with Moving Wings Glitter Butterfly Hair Clip for Women and Girls\", \"brand\": \"SIQUK\", \"main_category\": \"All Beauty\", \"main_image\": \"https://m.media-amazon.com/images/I/819fTS6BkyL._SL1500_.jpg\", \"generated\": {\"intents\": [\"Colorful butterfly clips for hair styling\", \"Glittery clips with movable wings for girls\", \"Decorative hair accessories with butterfly design\", \"90s style hair clip...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filters synthetic_intents.jsonl using meta_All_Beauty.jsonl\n",
        "# Adds a report that ALWAYS includes the two policy rules even if counts are 0.\n",
        "\n",
        "import json, re, math, pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "META_PATH   = \"/content/summarized_for_review_merged_final.jsonl\"\n",
        "SYN_PATH    = \"/content/synthetic_intents.jsonl\"\n",
        "KEEP_PATH   = \"/content/filtered_synthetic_intents.jsonl\"\n",
        "DROP_PATH   = \"/content/filtered_synthetic_intents.dropped.jsonl\"\n",
        "REPORT_CSV  = \"/content/synthetic_filter_report.csv\"\n",
        "\n",
        "\n",
        "MISSING_STRINGS = {\n",
        "    \"\", \"none\", \"null\", \"na\", \"n/a\", \"nan\", \"nah\", \"unknown\", \"undefined\",\n",
        "    \"n\\\\a\", \"not available\", \"not applicable\", \"tbd\", \"to be determined\", \"nil\",\n",
        "    \"-\", \"—\", \"nill\", \"n/a.\", \"na.\", \"n.a.\", \"unk\", \"unspecified\"\n",
        "}\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def read_jsonl(path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                yield json.loads(line)\n",
        "\n",
        "def nows(s):\n",
        "    return (s if isinstance(s, str) else \"\").strip().lower()\n",
        "\n",
        "def is_missing(v):\n",
        "    if v is None:\n",
        "        return True\n",
        "    if isinstance(v, float) and math.isnan(v):\n",
        "        return True\n",
        "    if isinstance(v, str) and nows(v) in MISSING_STRINGS:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "_num_pat = re.compile(r\"[-+]?\\d*\\.?\\d+\")\n",
        "def to_float(v):\n",
        "    if v is None:\n",
        "        return None\n",
        "    if isinstance(v, (int, float)):\n",
        "        return float(v)\n",
        "    if isinstance(v, dict):\n",
        "        for k in [\"amount\", \"value\", \"price\", \"price_amount\"]:\n",
        "            if k in v:\n",
        "                return to_float(v[k])\n",
        "        return None\n",
        "    if isinstance(v, str):\n",
        "        s = v.strip().replace(\",\", \"\")\n",
        "        try:\n",
        "            return float(s)\n",
        "        except:\n",
        "            m = _num_pat.search(s)\n",
        "            if m:\n",
        "                try:\n",
        "                    return float(m.group(0))\n",
        "                except:\n",
        "                    return None\n",
        "    return None\n",
        "\n",
        "def normalize_brand(s):\n",
        "    if not isinstance(s, str):\n",
        "        return None\n",
        "    s = s.lower().strip()\n",
        "    s = re.sub(r\"[^a-z0-9\\s]\", \"\", s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s if s else None\n",
        "\n",
        "def extract_meta_fields(rec):\n",
        "    details = rec.get(\"details\") or {}\n",
        "    brand = details.get(\"Brand\") or rec.get(\"brand\") or rec.get(\"store\")\n",
        "    price = rec.get(\"price\")\n",
        "    rating = rec.get(\"avg_rating\", rec.get(\"average_rating\"))\n",
        "    brand_norm = None if is_missing(brand) else normalize_brand(brand)\n",
        "    price_val  = None if is_missing(price)  else to_float(price)\n",
        "    rating_val = None if is_missing(rating) else to_float(rating)\n",
        "    return brand_norm, price_val, rating_val\n",
        "\n",
        "def extract_constraint_fields(obj):\n",
        "    if not isinstance(obj, dict):\n",
        "        return None, None, None, None\n",
        "    brand_c  = obj.get(\"brand\")\n",
        "    pmin_c   = obj.get(\"price_min\")\n",
        "    pmax_c   = obj.get(\"price_max\")\n",
        "    rmin_c   = obj.get(\"rating_min\")\n",
        "    brand_c_norm = None if is_missing(brand_c) else normalize_brand(brand_c)\n",
        "    pmin_val     = None if is_missing(pmin_c) else to_float(pmin_c)\n",
        "    pmax_val     = None if is_missing(pmax_c) else to_float(pmax_c)\n",
        "    rmin_val     = None if is_missing(rmin_c) else to_float(rmin_c)\n",
        "    return brand_c_norm, pmin_val, pmax_val, rmin_val\n",
        "\n",
        "# ---------- load META ----------\n",
        "meta_map = {}\n",
        "for rec in read_jsonl(META_PATH):\n",
        "    asin = rec.get(\"parent_asin\") or rec.get(\"asin\") or rec.get(\"id\")\n",
        "    if not asin:\n",
        "        continue\n",
        "    brand_norm, price_val, rating_val = extract_meta_fields(rec)\n",
        "    meta_map[asin] = {\"brand_norm\": brand_norm, \"price\": price_val, \"rating\": rating_val}\n",
        "\n",
        "# ---------- filtering ----------\n",
        "keep_f = open(KEEP_PATH, \"w\", encoding=\"utf-8\")\n",
        "drop_f = open(DROP_PATH, \"w\", encoding=\"utf-8\")\n",
        "reason_counts = Counter()\n",
        "dropped_examples = []\n",
        "\n",
        "def constraints_satisfy_meta(constraint_obj, meta_fields):\n",
        "    reasons = []\n",
        "    brand_m, price_m, rating_m = meta_fields[\"brand_norm\"], meta_fields[\"price\"], meta_fields[\"rating\"]\n",
        "    brand_c, pmin_c, pmax_c, rmin_c = extract_constraint_fields(constraint_obj)\n",
        "\n",
        "    # BRAND (if meta has brand)\n",
        "    if brand_m is not None:\n",
        "        if brand_c is None:\n",
        "            reasons.append(\"brand_missing\")\n",
        "        elif brand_c != brand_m:\n",
        "            reasons.append(\"brand_mismatch\")\n",
        "    # PRICE (if meta has price)\n",
        "    if price_m is not None:\n",
        "        if pmin_c is None or pmax_c is None:\n",
        "            reasons.append(\"price_missing\")\n",
        "        else:\n",
        "            if pmin_c > pmax_c:\n",
        "                pmin_c, pmax_c = pmax_c, pmin_c\n",
        "            if not (pmin_c <= price_m <= pmax_c):\n",
        "                reasons.append(\"price_mismatch\")\n",
        "    # RATING (if meta has rating)\n",
        "    if rating_m is not None:\n",
        "        if rmin_c is None:\n",
        "            reasons.append(\"rating_missing\")\n",
        "        else:\n",
        "            if rmin_c > rating_m:\n",
        "                reasons.append(\"rating_mismatch\")\n",
        "\n",
        "    return (len(reasons) == 0, reasons)\n",
        "\n",
        "total = kept = dropped = 0\n",
        "\n",
        "for rec in read_jsonl(SYN_PATH):\n",
        "    total += 1\n",
        "    asin = rec.get(\"parent_asin\") or rec.get(\"asin\") or rec.get(\"id\")\n",
        "\n",
        "    if asin not in meta_map:\n",
        "        keep_f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
        "        kept += 1\n",
        "        continue\n",
        "\n",
        "    meta_fields = meta_map[asin]\n",
        "    gen = rec.get(\"generated\") or {}\n",
        "    cons = gen.get(\"constraints\")\n",
        "    constraints_list = cons if isinstance(cons, list) else ([cons] if isinstance(cons, dict) else [])\n",
        "\n",
        "    meta_has_any = any(v is not None for v in [meta_fields[\"brand_norm\"], meta_fields[\"price\"], meta_fields[\"rating\"]])\n",
        "\n",
        "    if meta_has_any and not constraints_list:\n",
        "        reasons = []\n",
        "        if meta_fields[\"brand_norm\"] is not None: reasons.append(\"brand_missing\")\n",
        "        if meta_fields[\"price\"]      is not None: reasons.append(\"price_missing\")\n",
        "        if meta_fields[\"rating\"]     is not None: reasons.append(\"rating_missing\")\n",
        "        reason_counts.update(reasons)\n",
        "        rec_copy = dict(rec)\n",
        "        rec_copy[\"_filter_reasons\"] = reasons\n",
        "        drop_f.write(json.dumps(rec_copy, ensure_ascii=False) + \"\\n\")\n",
        "        dropped += 1\n",
        "        if len(dropped_examples) < 5:\n",
        "            dropped_examples.append((asin, reasons))\n",
        "        continue\n",
        "\n",
        "    any_valid = False\n",
        "    all_reasons_aggregate = []\n",
        "    for cobj in constraints_list:\n",
        "        ok, reasons = constraints_satisfy_meta(cobj, meta_fields)\n",
        "        if ok:\n",
        "            any_valid = True\n",
        "            break\n",
        "        else:\n",
        "            all_reasons_aggregate.extend(reasons)\n",
        "\n",
        "    if any_valid:\n",
        "        keep_f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
        "        kept += 1\n",
        "    else:\n",
        "        if not all_reasons_aggregate and meta_has_any:\n",
        "            all_reasons_aggregate = [\"unspecified_mismatch\"]\n",
        "        reason_counts.update(all_reasons_aggregate)\n",
        "        rec_copy = dict(rec)\n",
        "        rec_copy[\"_filter_reasons\"] = sorted(set(all_reasons_aggregate))\n",
        "        drop_f.write(json.dumps(rec_copy, ensure_ascii=False) + \"\\n\")\n",
        "        dropped += 1\n",
        "        if len(dropped_examples) < 5:\n",
        "            dropped_examples.append((asin, sorted(set(all_reasons_aggregate))))\n",
        "\n",
        "keep_f.close()\n",
        "drop_f.close()\n",
        "\n",
        "# ---------- build ALWAYS-ON report ----------\n",
        "# Ensure zero entries exist so they appear even when no drops occurred.\n",
        "for k in [\"brand_missing\", \"price_missing\", \"rating_missing\",\n",
        "          \"brand_mismatch\", \"price_mismatch\", \"rating_mismatch\",\n",
        "          \"unspecified_mismatch\"]:\n",
        "    _ = reason_counts[k]  # touch to create with 0 if absent\n",
        "\n",
        "RULE_ROWS = [\n",
        "    {\n",
        "        \"reason\": \"RULE_1_OMISSION\",\n",
        "        \"count\": (reason_counts[\"brand_missing\"] +\n",
        "                  reason_counts[\"price_missing\"] +\n",
        "                  reason_counts[\"rating_missing\"]),\n",
        "        \"explanation\": \"If META has a real value for brand/rating/price but the constraint omits that field -> DROP\"\n",
        "    },\n",
        "    {\n",
        "        \"reason\": \"RULE_2_MISMATCH\",\n",
        "        \"count\": (reason_counts[\"brand_mismatch\"] +\n",
        "                  reason_counts[\"price_mismatch\"] +\n",
        "                  reason_counts[\"rating_mismatch\"]),\n",
        "        \"explanation\": \"If META has a real value and the constraint is present but doesn't match: brand/rating/price\"\n",
        "    },\n",
        "    # Sub-reasons (always shown, even if 0)\n",
        "    {\"reason\": \"brand_missing\",  \"count\": reason_counts[\"brand_missing\"],\n",
        "     \"explanation\": \"META brand present, constraint 'brand' missing -> DROP\"},\n",
        "    {\"reason\": \"price_missing\",  \"count\": reason_counts[\"price_missing\"],\n",
        "     \"explanation\": \"META price present, constraint price_min/price_max missing -> DROP\"},\n",
        "    {\"reason\": \"rating_missing\", \"count\": reason_counts[\"rating_missing\"],\n",
        "     \"explanation\": \"META rating present, constraint rating_min missing -> DROP\"},\n",
        "    {\"reason\": \"brand_mismatch\", \"count\": reason_counts[\"brand_mismatch\"],\n",
        "     \"explanation\": \"Normalized constraint brand != normalized META brand -> DROP\"},\n",
        "    {\"reason\": \"price_mismatch\", \"count\": reason_counts[\"price_mismatch\"],\n",
        "     \"explanation\": \"META price not inside [price_min, price_max] -> DROP\"},\n",
        "    {\"reason\": \"rating_mismatch\",\"count\": reason_counts[\"rating_mismatch\"],\n",
        "     \"explanation\": \"rating_min > META avg_rating -> DROP\"},\n",
        "]\n",
        "\n",
        "# Include any other reasons that might have been used\n",
        "other_rows = []\n",
        "for r, c in reason_counts.items():\n",
        "    if r not in {row[\"reason\"] for row in RULE_ROWS}:\n",
        "        other_rows.append({\"reason\": r, \"count\": c, \"explanation\": \"\"})\n",
        "\n",
        "report_df = pd.DataFrame(RULE_ROWS + other_rows)\n",
        "report_df.to_csv(REPORT_CSV, index=False)\n",
        "\n",
        "print(f\"Total: {total} | Kept: {kept} | Dropped: {dropped}\")\n",
        "print(f\"Kept -> {KEEP_PATH}\")\n",
        "print(f\"Dropped -> {DROP_PATH}\")\n",
        "print(f\"Report -> {REPORT_CSV}\")\n",
        "if dropped_examples:\n",
        "    print(\"Examples of dropped [asin, reasons]:\")\n",
        "    for asin, rs in dropped_examples:\n",
        "        print(\" -\", asin, rs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrIVvUgPprOu",
        "outputId": "ad0c9d5a-f0c9-49b1-fd36-67a03c328c8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total: 3000 | Kept: 2725 | Dropped: 275\n",
            "Kept -> /content/filtered_synthetic_intents.jsonl\n",
            "Dropped -> /content/filtered_synthetic_intents.dropped.jsonl\n",
            "Report -> /content/synthetic_filter_report.csv\n",
            "Examples of dropped [asin, reasons]:\n",
            " - B01ESPRJEC ['brand_missing']\n",
            " - B088GMVYQX ['brand_missing']\n",
            " - B09XK4NNGW ['brand_missing']\n",
            " - B08XB7PK7B ['brand_missing']\n",
            " - B09BN5KT7S ['brand_missing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "emtv_YDxsxJr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}